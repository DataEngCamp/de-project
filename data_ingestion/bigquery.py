"""
BigQuery æ“ä½œæ¨¡çµ„
åƒè€ƒ mysql.py çš„çµæ§‹ï¼Œæä¾› BigQuery çš„è³‡æ–™åº«æ“ä½œåŠŸèƒ½
"""
import pandas as pd
from google.cloud import bigquery
from google.cloud.bigquery import SchemaField, LoadJobConfig, WriteDisposition
import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from data_ingestion.config import GCP_PROJECT_ID as PROJECT_ID

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# BigQuery é…ç½®
DATASET_ID = "hahow_analytics"

def get_bigquery_client():
    """å»ºç«‹ BigQuery å®¢æˆ¶ç«¯"""
    return bigquery.Client(project=PROJECT_ID)

def create_dataset_if_not_exists(dataset_id: str = DATASET_ID):
    """å»ºç«‹ BigQuery Datasetï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    client = get_bigquery_client()
    dataset_ref = client.dataset(dataset_id)
    
    try:
        client.get_dataset(dataset_ref)
        logger.info(f"Dataset {dataset_id} already exists")
    except Exception:
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "US"  # æˆ–æ‚¨åå¥½çš„å€åŸŸ
        dataset = client.create_dataset(dataset)
        logger.info(f"Created dataset {dataset_id}")

# ========== Table Schema å®šç¾© ==========

def hahow_course_bq_schema():
    """å®šç¾© hahow_course è¡¨çš„ BigQuery schema"""
    schema = [
        bigquery.SchemaField(name="id", field_type="STRING"),
        bigquery.SchemaField(name="category", field_type="STRING"),
        bigquery.SchemaField(name="uniquename", field_type="STRING"),
        bigquery.SchemaField(name="title", field_type="STRING"),
        bigquery.SchemaField(name="status", field_type="STRING"),
        bigquery.SchemaField(name="link", field_type="STRING"),
        bigquery.SchemaField(name="price", field_type="NUMERIC"),
        bigquery.SchemaField(name="preordered_price", field_type="NUMERIC"),
        bigquery.SchemaField(name="average_rating", field_type="FLOAT"),
        bigquery.SchemaField(name="num_rating", field_type="INTEGER"),
        bigquery.SchemaField(name="owner_name", field_type="STRING"),
        bigquery.SchemaField(name="sold_num", field_type="INTEGER"),
        bigquery.SchemaField(name="bookmark_count", field_type="INTEGER"),
        bigquery.SchemaField(name="meta_description", field_type="STRING"),
        bigquery.SchemaField(name="cover_image", field_type="STRING"),
        bigquery.SchemaField(name="incubate_time", field_type="TIMESTAMP"),
        bigquery.SchemaField(name="publish_time", field_type="TIMESTAMP"),
        bigquery.SchemaField(name="video_length", field_type="INTEGER"),
        bigquery.SchemaField(name="uploaded_at", field_type="TIMESTAMP"),
    ]
    return schema

def hahow_course_sales_bq_schema():
    """å®šç¾© hahow_course_sales è¡¨çš„ BigQuery schema"""
    schema = [
        bigquery.SchemaField(name="id", field_type="INTEGER"),
        bigquery.SchemaField(name="course_id", field_type="STRING"),
        bigquery.SchemaField(name="price", field_type="NUMERIC"),
        bigquery.SchemaField(name="sold_num", field_type="INTEGER"),
        bigquery.SchemaField(name="captured_at", field_type="TIMESTAMP"),
        bigquery.SchemaField(name="uploaded_at", field_type="TIMESTAMP"),
    ]
    return schema

def hahow_article_bq_schema():
    """å®šç¾© hahow_article è¡¨çš„ BigQuery schema"""
    schema = [
        bigquery.SchemaField(name="id", field_type="STRING"),
        bigquery.SchemaField(name="category", field_type="STRING"),
        bigquery.SchemaField(name="type", field_type="STRING"),
        bigquery.SchemaField(name="title", field_type="STRING"),
        bigquery.SchemaField(name="group_title", field_type="STRING"),
        bigquery.SchemaField(name="group_uniquename", field_type="STRING"),
        bigquery.SchemaField(name="subgroup_title", field_type="STRING"),
        bigquery.SchemaField(name="subgroup_uniquename", field_type="STRING"),
        bigquery.SchemaField(name="link", field_type="STRING"),
        bigquery.SchemaField(name="tags", field_type="STRING"),
        bigquery.SchemaField(name="creator_name", field_type="STRING"),
        bigquery.SchemaField(name="view_count", field_type="INTEGER"),
        bigquery.SchemaField(name="clap_total", field_type="INTEGER"),
        bigquery.SchemaField(name="preview_description", field_type="STRING"),
        bigquery.SchemaField(name="cover_image", field_type="STRING"),
        bigquery.SchemaField(name="created_at", field_type="TIMESTAMP"),
        bigquery.SchemaField(name="updated_at", field_type="TIMESTAMP"),
        bigquery.SchemaField(name="publish_at", field_type="TIMESTAMP"),
        bigquery.SchemaField(name="uploaded_at", field_type="TIMESTAMP"),
    ]
    return schema

# ========== åŸºç¤æ“ä½œå‡½æ•¸ ==========

def create_table(table_name: str, schema: List[SchemaField], dataset_id: str = DATASET_ID):
    """
    å»ºç«‹ BigQuery è¡¨
    
    Args:
        table_name: è¡¨å
        schema: BigQuery schema
        dataset_id: Dataset ID
    """
    client = get_bigquery_client()
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    # ç¢ºä¿ Dataset å­˜åœ¨
    create_dataset_if_not_exists(dataset_id)
    
    table = bigquery.Table(table_id, schema=schema)
    
    try:
        table = client.create_table(table)
        print(f"âœ… å»ºç«‹ BigQuery è¡¨ '{table_name}' æˆåŠŸ")
    except Exception as e:
        if "already exists" in str(e).lower():
            print(f"â„¹ï¸  BigQuery è¡¨ '{table_name}' å·²å­˜åœ¨")
        else:
            print(f"âŒ å»ºç«‹ BigQuery è¡¨ '{table_name}' å¤±æ•—: {e}")
            raise

def upload_data_to_bigquery(table_name: str, df: pd.DataFrame, dataset_id: str = DATASET_ID, mode: str = "replace"):
    """
    ä¸Šå‚³ DataFrame åˆ° BigQueryï¼ˆé¡ä¼¼ mysql.py çš„ upload_data_to_mysqlï¼‰
    
    Args:
        table_name: è¡¨å
        df: è¦ä¸Šå‚³çš„ DataFrame
        dataset_id: Dataset ID
        mode: å¯«å…¥æ¨¡å¼ ("replace", "append")
    """
    client = get_bigquery_client()
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    # ç¢ºä¿ Dataset å­˜åœ¨
    create_dataset_if_not_exists(dataset_id)
    
    # è¨­å®šå¯«å…¥æ¨¡å¼
    if mode == "replace":
        write_disposition = WriteDisposition.WRITE_TRUNCATE
    elif mode == "append":
        write_disposition = WriteDisposition.WRITE_APPEND
    else:
        write_disposition = WriteDisposition.WRITE_EMPTY
    
    # é…ç½®è¼‰å…¥å·¥ä½œ
    job_config = LoadJobConfig(
        write_disposition=write_disposition,
        autodetect=True,  # è‡ªå‹•åµæ¸¬ schema
    )
    
    try:
        # åŸ·è¡Œè¼‰å…¥
        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
        job.result()  # ç­‰å¾…å®Œæˆ
        
        print(f"âœ… è³‡æ–™å·²ä¸Šå‚³åˆ° BigQuery è¡¨ '{table_name}'ï¼Œå…± {len(df)} ç­†è¨˜éŒ„")
        
    except Exception as e:
        print(f"âŒ ä¸Šå‚³è³‡æ–™åˆ° BigQuery è¡¨ '{table_name}' å¤±æ•—: {e}")
        raise

def upload_data_to_bigquery_insert(table_name: str, data: List[Dict], dataset_id: str = DATASET_ID):
    """
    ä½¿ç”¨ insert_rows_json ä¸Šå‚³è³‡æ–™åˆ° BigQueryï¼ˆé¡ä¼¼ mysql.py çš„ upload_data_to_mysql_insertï¼‰
    
    Args:
        table_name: è¡¨å
        data: è¦æ’å…¥çš„è³‡æ–™åˆ—è¡¨
        dataset_id: Dataset ID
    """
    client = get_bigquery_client()
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    # ç¢ºä¿ Dataset å­˜åœ¨
    create_dataset_if_not_exists(dataset_id)
    
    try:
        # ç²å–è¡¨å¼•ç”¨
        table = client.get_table(table_id)
        
        # ç›´æ¥æ’å…¥è³‡æ–™
        errors = client.insert_rows_json(table, data)
        
        if errors:
            print(f"âŒ æ’å…¥è³‡æ–™åˆ° BigQuery è¡¨ '{table_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {errors}")
            raise Exception(f"BigQuery æ’å…¥å¤±æ•—: {errors}")
        
        print(f"âœ… INSERT å®Œæˆï¼Œè™•ç† {len(data)} ç­†è¨˜éŒ„åˆ° BigQuery è¡¨ '{table_name}'")
        
    except Exception as e:
        print(f"âŒ æ’å…¥è³‡æ–™åˆ° BigQuery è¡¨ '{table_name}' å¤±æ•—: {e}")
        raise

def create_view(view_name: str, view_sql: str, dataset_id: str = DATASET_ID):
    """
    åœ¨ BigQuery ä¸­å»ºç«‹æˆ–æ›¿æ› Viewï¼ˆé¡ä¼¼ mysql.py çš„ create_viewï¼‰
    
    Args:
        view_name: View çš„åç¨±
        view_sql: å»ºç«‹ View çš„ SQL èªå¥
        dataset_id: Dataset ID
    """
    client = get_bigquery_client()
    view_id = f"{PROJECT_ID}.{dataset_id}.{view_name}"
    
    # ç¢ºä¿ Dataset å­˜åœ¨
    create_dataset_if_not_exists(dataset_id)
    
    view = bigquery.Table(view_id)
    view.view_query = view_sql
    
    try:
        # å˜—è©¦æ›´æ–°ç¾æœ‰ view
        client.update_table(view, ["view_query"])
        print(f"âœ… æ›´æ–° BigQuery View '{view_name}' æˆåŠŸ")
    except Exception:
        # å¦‚æœä¸å­˜åœ¨å‰‡å»ºç«‹æ–°çš„
        try:
            view = client.create_table(view)
            print(f"âœ… å»ºç«‹ BigQuery View '{view_name}' æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å»ºç«‹ BigQuery View '{view_name}' å¤±æ•—: {e}")
            raise

def create_table_from_view(view_name: str, table_name: str, dataset_id: str = DATASET_ID):
    """
    å¾ BigQuery View å»ºç«‹å¯¦é«” Tableï¼ˆé¡ä¼¼ mysql.py çš„ create_table_from_viewï¼‰
    
    Args:
        view_name: ä¾†æº View çš„åç¨±
        table_name: ç›®æ¨™ Table çš„åç¨±
        dataset_id: Dataset ID
    """
    client = get_bigquery_client()
    
    source_view_id = f"{PROJECT_ID}.{dataset_id}.{view_name}"
    dest_table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    try:
        # å®Œå…¨å–ä»£ï¼šå…ˆåˆªé™¤èˆŠ Tableï¼Œå†å»ºç«‹æ–°çš„
        print(f"ğŸ—‘ï¸  æ­£åœ¨åˆªé™¤èˆŠçš„ BigQuery Table '{table_name}' (å¦‚æœå­˜åœ¨)...")
        try:
            client.delete_table(dest_table_id)
        except Exception:
            pass  # è¡¨ä¸å­˜åœ¨æ™‚å¿½ç•¥éŒ¯èª¤
        
        print(f"ğŸ“ æ­£åœ¨å¾ View '{view_name}' å»ºç«‹æ–°çš„ BigQuery Table '{table_name}'...")
        
        # å»ºç«‹æŸ¥è©¢å·¥ä½œä¾†è¤‡è£½ view è³‡æ–™åˆ° table
        sql = f"SELECT * FROM `{source_view_id}`"
        
        job_config = bigquery.QueryJobConfig(
            destination=dest_table_id,
            write_disposition=WriteDisposition.WRITE_TRUNCATE
        )
        
        query_job = client.query(sql, job_config=job_config)
        query_job.result()  # ç­‰å¾…å®Œæˆ
        
        # ç²å–è¨˜éŒ„æ•¸é‡
        count_sql = f"SELECT COUNT(*) as count FROM `{dest_table_id}`"
        count_result = client.query(count_sql).result()
        count = list(count_result)[0].count
        
        print(f"âœ… æˆåŠŸå»ºç«‹ BigQuery Table '{table_name}'ï¼Œå…± {count} ç­†è¨˜éŒ„")
        
    except Exception as e:
        print(f"âŒ å¾ View '{view_name}' å»ºç«‹ BigQuery Table '{table_name}' å¤±æ•—: {e}")
        raise

# ========== å…·é«”æ¥­å‹™é‚è¼¯å‡½æ•¸ ==========

def create_course_sales_daily_view():
    """
    å»ºç«‹èª²ç¨‹éŠ·å”®æ—¥çµ±è¨ˆ Viewï¼ˆå°æ‡‰ mysql.py çš„åŒåå‡½æ•¸ï¼‰
    """
    view_sql = f"""
    CREATE OR REPLACE VIEW `{PROJECT_ID}.{DATASET_ID}.vw_course_sales_daily` AS
    SELECT
      t.course_id,
      DATE(t.captured_at) AS captured_date,
      t.price,
      t.sold_num,
      t.price * t.sold_num AS revenue
    FROM (
      SELECT
        s.*,
        ROW_NUMBER() OVER (
          PARTITION BY s.course_id, DATE(s.captured_at)
          ORDER BY s.sold_num DESC, s.captured_at DESC, s.id DESC
        ) AS rn
      FROM `{PROJECT_ID}.{DATASET_ID}.hahow_course_sales` s
      WHERE
        s.price < 999999
    ) AS t
    WHERE t.rn = 1
    """
    
    create_view("vw_course_sales_daily", view_sql)

# ========== è¼”åŠ©å‡½æ•¸ ==========

def execute_query(sql: str, dataset_id: str = DATASET_ID) -> List[Dict]:
    """
    åŸ·è¡Œ BigQuery SQL æŸ¥è©¢ä¸¦è¿”å›çµæœ
    
    Args:
        sql: SQL æŸ¥è©¢èªå¥
        dataset_id: Dataset ID
    
    Returns:
        æŸ¥è©¢çµæœçš„åˆ—è¡¨
    """
    client = get_bigquery_client()
    
    query_job = client.query(sql)
    results = query_job.result()
    
    # ç›´æ¥è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
    return [dict(row) for row in results]

def get_table_info(table_name: str, dataset_id: str = DATASET_ID) -> Dict:
    """
    ç²å– BigQuery è¡¨çš„è³‡è¨Š
    
    Args:
        table_name: è¡¨å
        dataset_id: Dataset ID
    
    Returns:
        è¡¨çš„è³‡è¨Šå­—å…¸
    """
    client = get_bigquery_client()
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    try:
        table = client.get_table(table_id)
        return {
            "table_id": table.table_id,
            "num_rows": table.num_rows,
            "num_bytes": table.num_bytes,
            "created": table.created,
            "modified": table.modified,
            "schema": [{"name": field.name, "type": field.field_type} for field in table.schema]
        }
    except Exception as e:
        print(f"âŒ ç²å–è¡¨ '{table_name}' è³‡è¨Šå¤±æ•—: {e}")
        raise

